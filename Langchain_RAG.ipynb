{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvp1tj7e1dEeKtYPyhHnkP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/exala/PIAIC_Q3/blob/main/Langchain_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is Retrieval-Augmented Generation (RAG)?**\n",
        "**Retrieval-Augmented Generation (RAG)** is an advanced AI framework that blends the power of information retrieval with generative models to deliver accurate, context-aware responses. By retrieving relevant documents from a database or knowledge source, RAG enhances the generative model's ability to produce ***reliable*** and ***domain-specific*** answers. Its purpose is to improve the quality of AI-generated content, making it especially effective for tasks requiring detailed knowledge or up-to-date information.\n",
        "\n",
        "---\n",
        "\n",
        "## **Use cases:**\n",
        "RAG is ideal for building intelligent Q&A systems, streamlining knowledge retrieval, and generating personalized insights from vast datasets. It enhances applications in customer support, education, and enterprise knowledge management."
      ],
      "metadata": {
        "id": "u-v7pDaoSyKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generative AI with Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "This project demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline for question-answering using LangChain, Pinecone, and Google Generative AI (Gemini). The workflow involves embedding documents, storing them in a Pinecone vector database, and querying them with a generative AI model.\n",
        "\n",
        "---\n",
        "\n",
        "## **Prerequisites**\n",
        "\n",
        "Before running the notebook, ensure you have the following:\n",
        "\n",
        "1. A **Pinecone** account with API keys and serverless settings.\n",
        "2. A **Google Generative AI** API key.\n",
        "3. A text or PDF file containing the documents you want to process (e.g., `fusionenergy1.txt`).\n",
        "\n",
        "---\n",
        "\n",
        "##**Installation**\n",
        "\n",
        "Run the following command to install the required libraries:\n"
      ],
      "metadata": {
        "id": "hC0t7_pNQQWf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPm1z4q3nF5b"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain pinecone-client google-generativeai openai tqdm langchain_google_genai langchain_community langchain-pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Environment Setup**\n",
        "Retrieve and set API keys for Pinecone and Google Generative AI using Colab's userdata:"
      ],
      "metadata": {
        "id": "GtMj7SyuRg2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "PINECONE_API_KEY = userdata.get(\"PINECONE_API_KEY\")\n",
        "# PINECONE_ENVIRONMENT = userdata.get(\"PINECONE_ENVIRONMENT\")\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "# os.environ[\"PINECONE_ENVIRONMENT\"] = PINECONE_ENVIRONMENT\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "# os.environ: Adds API keys and settings as environment variables, making them accessible across the script."
      ],
      "metadata": {
        "id": "bVrwaortp7Iq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Initialize Pinecone**\n",
        "Set up the Pinecone client and create an index:"
      ],
      "metadata": {
        "id": "g52BVi7gRlSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(\n",
        "    api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
        ") # Pinecone(api_key=...): Initializes the Pinecone client using the API key stored in the environment.\n",
        "\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)\n",
        "\n",
        "index_name = \"first-rag-oriject\"\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# See that it is empty\n",
        "print(\"Index before upsert:\")\n",
        "print(pc.Index(index_name).describe_index_stats())\n",
        "print(\"\\n\")\n",
        "index = pc.Index(index_name)\n",
        "print('index\\n', index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wR-Y7-tqpTd",
        "outputId": "b06fb089-85f6-4b0c-b799-3dda0cf6a999"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index before upsert:\n",
            "{'dimension': 768,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0}\n",
            "\n",
            "\n",
            "index\n",
            " <pinecone.data.index.Index object at 0x7854b0bd4a60>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Generate Embeddings**\n",
        "Use Google Generative AI Embeddings to create vector embeddings for your documents:"
      ],
      "metadata": {
        "id": "zDCyW2t9R1VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        "    )\n",
        "# GoogleGenerativeAIEmbeddings: A class that provides embeddings using Google's Generative AI."
      ],
      "metadata": {
        "id": "mLUz5O31u7CX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Document Preparation**\n",
        "Load and split your document(s) into smaller chunks for better retrieval performance:"
      ],
      "metadata": {
        "id": "eCrK6oH_R8Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = TextLoader(\"/content/ai.txt\")  # Replace with your document file.This loads the content of a text file.\n",
        "documents = loader.load() # Reads the content into a document object.\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) # Each chunk will contain up to 500 characters. Ensures a 50-character overlap between chunks to retain context.\n",
        "docs = text_splitter.split_documents(documents) # Splits the loaded document into smaller chunks."
      ],
      "metadata": {
        "id": "2fiCc83Kv_8M"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional (for PDFs):**"
      ],
      "metadata": {
        "id": "_6Eb0jb-SDvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### FOR PDFs\n",
        "# !pip install -q pypdf\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# # Use PyPDFLoader specifically for PDF files\n",
        "# loader = PyPDFLoader(\"/content/fusionenergy.pdf\")\n",
        "# documents = loader.load()\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "# docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "DGKs5CJryUKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Insert Data into Pinecone**\n",
        "Embed document chunks into vectors and upsert them into Pinecone:"
      ],
      "metadata": {
        "id": "_s-jR6DjSNYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Shows a progress bar\n",
        "\n",
        "for doc in tqdm(docs):  # Loop through document chunks\n",
        "    vector = embeddings.embed_query(doc.page_content)  # Generate vector\n",
        "    index.upsert([{\n",
        "        \"id\": doc.metadata[\"source\"],  # Use \"id\" instead of the first tuple element\n",
        "        \"values\": vector,  # Use \"values\" for the vector\n",
        "        \"metadata\": {\n",
        "            \"text\": doc.page_content,  # Add the text as part of metadata\n",
        "            \"source\": doc.metadata[\"source\"]  # Include the source\n",
        "        }\n",
        "        }])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKRWpWj2zUQI",
        "outputId": "3d234917-b0ae-4967-c97a-2d43755b0db3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:09<00:00,  2.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Configure the Generative AI Model**\n",
        "Set up the Google Generative AI model for generating answers:"
      ],
      "metadata": {
        "id": "HcRiiAVlSQlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# ChatGoogleGenerativeAI: A class for interacting with Google's Generative AI in chat mode.\n",
        "\n",
        "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=GOOGLE_API_KEY, temperature=1)"
      ],
      "metadata": {
        "id": "Nytal4h88KDT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Set Up Retrieval-Based QA**\n",
        "Create a retriever using the Pinecone vector store and set up a QA chain:"
      ],
      "metadata": {
        "id": "_bw6NDrjSVf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore # Connects LangChain to the Pinecone index for retrieval.\n",
        "from langchain.chains import RetrievalQA # A chain that combines retrieval and generative answering.\n",
        "\n",
        "# Create a vector store using the Pinecone index\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings # Ensures queries use the same embedding model.\n",
        ")\n",
        "\n",
        "# Create the retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})  # Retrieve top 4 most similar documents\n",
        "\n",
        "# Create the QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gemini_model,\n",
        "    chain_type=\"stuff\", # A simple chain that processes retrieved documents and generates answers.\n",
        "    retriever=retriever,  # Pass the retriever here\n",
        "    return_source_documents=True  # Optional: to get the source documents used in the response\n",
        ")"
      ],
      "metadata": {
        "id": "hF1gFl0f9fTY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Query the QA System**\n",
        "Ask a question and receive an answer from the RAG system:"
      ],
      "metadata": {
        "id": "Z-4D5NitSc7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is Fusion?\"\n",
        "response = qa_chain.invoke(query)\n",
        "print(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D5VSY5Z_mKu",
        "outputId": "0358ef36-92b9-4347-f639-dd9fac310798"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}